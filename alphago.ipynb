{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13411089,"sourceType":"datasetVersion","datasetId":8511255},{"sourceId":13452573,"sourceType":"datasetVersion","datasetId":8539106},{"sourceId":13826184,"sourceType":"datasetVersion","datasetId":8805180},{"sourceId":657151,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":496753,"modelId":512137}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# A replication of methods used in the AlphaGo paper \"Mastering the game of Go with deep neural networks and tree search","metadata":{}},{"cell_type":"code","source":"!pip install -q line_profiler\n%load_ext line_profiler\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Game Environment\n\nA Python-based game env taken from https://github.com/maxpumperla/deep_learning_and_the_game_of_go\n","metadata":{}},{"cell_type":"markdown","source":"### Point, Player","metadata":{}},{"cell_type":"code","source":"import enum\nfrom collections import namedtuple\n\nclass Point(namedtuple('Point', 'row col')):\n    def neighbors(self):\n        return [\n            Point(self.row - 1, self.col),\n            Point(self.row + 1, self.col),\n            Point(self.row, self.col - 1),\n            Point(self.row, self.col + 1),\n        ]\nclass Player(enum.Enum):\n    black = 1\n    white = 2\n\n    @property\n    def other(self):\n        return Player.black if self == Player.white else Player.white","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### GameResult, Territory","metadata":{}},{"cell_type":"code","source":"from __future__ import absolute_import\nfrom collections import namedtuple\n\nclass Territory(object):\n    def __init__(self, territory_map):\n        self.num_black_territory = 0\n        self.num_white_territory = 0\n        self.num_black_stones = 0\n        self.num_white_stones = 0\n        self.num_dame = 0\n        self.dame_points = []\n        for point, status in territory_map.items():\n            if status == Player.black:\n                self.num_black_stones += 1\n            elif status == Player.white:\n                self.num_white_stones += 1\n            elif status == 'territory_b':\n                self.num_black_territory += 1\n            elif status == 'territory_w':\n                self.num_white_territory += 1\n            elif status == 'dame':\n                self.num_dame += 1\n                self.dame_points.append(point)\n\nclass GameResult(namedtuple('GameResult', 'b w komi')):\n    @property\n    def winner(self):\n        if self.b > self.w + self.komi:\n            return Player.black\n        if self.b < self.w + self.komi:\n            return Player.white\n        return None\n\n    @property\n    def winning_margin(self):\n        w = self.w + self.komi\n        return abs(self.b - w)\n\n    def __str__(self):\n        w = self.w + self.komi\n        if self.b > w:\n            return 'B+%.1f' % (self.b - w,)\n        return 'W+%.1f' % (w - self.b,)\n\ndef _collect_region(start_pos, board, visited=None):\n    if visited is None:\n        visited = {}\n    if start_pos in visited:\n        return [], set()\n    all_points = [start_pos]\n    all_borders = set()\n    visited[start_pos] = True\n    here = board.get(start_pos)\n    deltas = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n    for delta_r, delta_c in deltas:\n        next_p = Point(row=start_pos.row + delta_r, col=start_pos.col + delta_c)\n        if not board.is_on_grid(next_p):\n            continue\n        neighbor = board.get(next_p)\n        if neighbor == here:\n            points, borders = _collect_region(next_p, board, visited)\n            all_points += points\n            all_borders |= borders\n        else:\n            all_borders.add(neighbor)\n    return all_points, all_borders\n\ndef evaluate_territory(board):\n    status = {}\n    for r in range(1, board.num_rows + 1):\n        for c in range(1, board.num_cols + 1):\n            p = Point(row=r, col=c)\n            if p in status:\n                continue\n            stone = board.get(p)\n            if stone is not None:\n                status[p] = board.get(p)\n            else:\n                group, neighbors = _collect_region(p, board)\n                if len(neighbors) == 1:\n                    neighbor_stone = neighbors.pop()\n                    stone_str = 'b' if neighbor_stone == Player.black else 'w'\n                    fill_with = 'terrtory_' + stone_str\n                else:\n                    fill_with = 'dame'\n                for pos in group:\n                    status[pos] = fill_with\n    return Territory(status)\n\ndef compute_game_result(game_state):\n    territory = evaluate_territory(game_state.board)\n    return GameResult(\n        territory.num_black_territory + territory.num_black_stones,\n        territory.num_white_territory + territory.num_white_stones,\n        komi=7.5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### GameBoard, GameState, Move, GoString","metadata":{}},{"cell_type":"code","source":"import copy\n\nclass Board():\n    def __init__(self, num_rows, num_cols):\n        self.num_rows = num_rows\n        self.num_cols = num_cols\n        self._grid = {}\n        self._hash = EMPTY_BOARD\n\n    # replace string should be immutable, which means that it should return a new grid\n    def _replace_string(self, new_string):\n        for point in new_string.stones:\n            self._grid[point] = new_string\n\n    def _remove_string(self, string):\n        for point in string.stones:\n            for neighbor in point.neighbors():\n                neighbor_string = self._grid.get(neighbor)\n                if neighbor_string is None:\n                    continue\n                if neighbor_string is not string:\n                    self._replace_string(neighbor_string.with_liberty(point))\n            self._grid[point] = None\n            self._hash ^= HASH_CODE[point, string.color]\n\n    def zobrist_hash(self):\n        return self._hash\n\n    def place_stone(self, player, point):\n        assert self.is_on_grid(point)\n        assert self._grid.get(point) is None\n        adjacent_same_color = []\n        adjacent_opposite_color = []\n        liberties = []\n        for neighbor in point.neighbors():\n            if not self.is_on_grid(neighbor):\n                continue\n            neighbor_string = self._grid.get(neighbor)\n            if neighbor_string is None:\n                liberties.append(neighbor)\n            elif neighbor_string.color == player:\n                if neighbor_string not in adjacent_same_color:\n                    adjacent_same_color.append(neighbor_string)\n            else:\n                if neighbor_string not in adjacent_opposite_color:\n                    adjacent_opposite_color.append(neighbor_string)\n        new_string = GoString(player, [point], liberties)\n        for same_color_string in adjacent_same_color:\n            new_string = new_string.merged_with(same_color_string)\n        for new_string_point in new_string.stones:\n            self._grid[new_string_point] = new_string\n        self._hash ^= HASH_CODE[point, player]\n        for other_color_string in adjacent_opposite_color:\n            replacement = other_color_string.without_liberty(point)\n            if replacement.num_liberties:\n                self._replace_string(other_color_string.without_liberty(point))\n            else:\n                self._remove_string(other_color_string)\n\n    def is_on_grid(self, point):\n        return 1 <= point.row <= self.num_rows and \\\n            1 <= point.col <= self.num_cols\n\n    def get(self, point):\n        string = self._grid.get(point)\n        if string is None:\n            return None\n        return string.color\n\n    def get_go_string(self, point):\n        string = self._grid.get(point)\n        if string is None:\n            return None\n        return string\n\nclass Move():\n    def __init__(self, point=None, is_pass=False, is_resign=False):\n        assert (point is not None) ^ is_pass ^ is_resign\n        self.point = point\n        self.is_play = (self.point is not None)\n        self.is_pass = is_pass\n        self.is_resign = is_resign\n\n    @classmethod\n    def play(cls, point):\n        return Move(point=point)\n\n    @classmethod\n    def pass_turn(cls):\n        return Move(is_pass=True)\n\n    @classmethod\n    def resign(cls):\n        return Move(is_resign=True)\n\n# Chain of connected stones (used to e.g. efficiently check for liberties)\nclass GoString():\n    def __init__(self, color, stones, liberties):\n        self.color = color\n        self.stones = frozenset(stones)\n        self.liberties = frozenset(liberties)\n\n    def without_liberty(self, point):\n        new_liberties = self.liberties - set([point])\n        return GoString(self.color, self.stones, new_liberties)\n\n    def with_liberty(self, point):\n        new_liberties = self.liberties | set([point])\n        return GoString(self.color, self.stones, new_liberties)\n\n    def merged_with(self, go_string):\n        assert go_string.color == self.color\n        combined_stones = self.stones | go_string.stones\n        return GoString(\n            self.color,\n            combined_stones,\n            (self.liberties | go_string.liberties) - combined_stones\n        )\n\n    @property\n    def num_liberties(self):\n        return len(self.liberties)\n\n    def __eq__(self, other):\n        return isinstance(other, GoString) and \\\n            self.color == other.color and \\\n            self.stones == other.stones and \\\n            self.liberties == other.liberties\n\nclass GameState():\n    def __init__(self, board, next_player, previous, move):\n        self.board = board\n        self.next_player = next_player\n        self.previous_state = previous\n        if self.previous_state is None:\n            self.previous_states = frozenset()\n        else:\n            self.previous_states = frozenset(\n                previous.previous_states |\n                {(previous.next_player, previous.board.zobrist_hash())})\n        self.last_move = move\n\n    def apply_move(self, move):\n        if move.is_play:\n            next_board = copy.deepcopy(self.board)\n            next_board.place_stone(self.next_player, move.point)\n        else:\n            next_board = self.board\n        return GameState(next_board, self.next_player.other, self, move)\n\n    def is_over(self):\n        if self.last_move is None:\n            return False\n        if self.last_move.is_resign:\n            return True\n        second_last_move = self.previous_state.last_move\n        if second_last_move is None:\n            return False\n        return self.last_move.is_pass and second_last_move.is_pass\n\n    def is_move_self_capture(self, player, move):\n        if not move.is_play:\n            return False\n        next_board = copy.deepcopy(self.board)\n        next_board.place_stone(player, move.point)\n        new_string = next_board.get_go_string(move.point)\n        return new_string.num_liberties == 0\n\n    def is_valid_move(self, move):\n        if self.is_over():\n            return False\n        if move.is_pass or move.is_resign:\n            return True\n        return (\n            self.board.get(move.point) is None and\n            not self.is_move_self_capture(self.next_player, move) and\n            not self.does_move_violate_ko(self.next_player, move))\n    \n    def legal_moves(self):\n        if self.is_over():\n            return []\n        moves = []\n        for row in range(1, self.board.num_rows + 1):\n            for col in range(1, self.board.num_cols + 1):\n                move = Move.play(Point(row, col))\n                if self.is_valid_move(move):\n                    moves.append(move)\n        moves.append(Move.pass_turn())\n        moves.append(Move.resign())\n        return moves\n    \n    def winner(self):\n        if not self.is_over():\n            return None\n        if self.last_move.is_resign:\n            return self.next_player\n        game_result = compute_game_result(self)\n        return game_result.winner\n\n    @classmethod\n    def new_game(cls, board_size):\n        if isinstance(board_size, int):\n            board_size = (board_size, board_size)\n        board = Board(*board_size)\n        return GameState(board, Player.black, None, None)\n\n    @property\n    def situation(self):\n        return (self.next_player, self.board)\n\n    def does_move_violate_ko(self, player, move):\n        if not move.is_play:\n            return False\n        next_board = copy.deepcopy(self.board)\n        next_board.place_stone(player, move.point)\n        next_situation = (player.other, next_board.zobrist_hash())\n        return next_situation in self.previous_states","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Zobrist Hash","metadata":{}},{"cell_type":"code","source":"import random\n\nMAX63 = 0x7fffffffffffffff\n\nHASH_CODE = {}\nEMPTY_BOARD = 0\n\nfor row in range(1, 20):\n    for col in range(1, 20):\n        for state in (1, 2):\n            code = random.randint(0, MAX63)\n            HASH_CODE[Point(row, col), state] = code\n\nprint(HASH_CODE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fast Game Environment","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom collections import defaultdict\n\nclass FastBoard:\n    \"\"\"Optimized board using numpy arrays\"\"\"\n    def __init__(self, num_rows, num_cols):\n        self.num_rows = num_rows\n        self.num_cols = num_cols\n        # Use numpy arrays: 0=empty, 1=black, 2=white\n        self.grid = np.zeros((num_rows, num_cols), dtype=np.int8)\n        self._hash = 0\n        \n    def copy(self):\n        \"\"\"Fast shallow copy with numpy array copy\"\"\"\n        new_board = FastBoard.__new__(FastBoard)\n        new_board.num_rows = self.num_rows\n        new_board.num_cols = self.num_cols\n        new_board.grid = self.grid.copy()  # Fast numpy copy\n        new_board._hash = self._hash\n        return new_board\n    \n    def place_stone(self, player, row, col):\n        \"\"\"Place stone and handle captures\"\"\"\n        self.grid[row, col] = player\n        point = Point(row+1, col+1)\n        self._hash ^= HASH_CODE[point, player]\n        \n        # Check for captures of opponent stones\n        opponent = 3 - player  # Toggles between 1 and 2\n        for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n            nr, nc = row + dr, col + dc\n            if self._is_on_grid(nr, nc) and self.grid[nr, nc] == opponent:\n                if self._count_liberties(nr, nc) == 0:\n                    self._remove_group(nr, nc)\n    \n    def _is_on_grid(self, row, col):\n        return 0 <= row < self.num_rows and 0 <= col < self.num_cols\n    \n    def _count_liberties(self, row, col):\n        \"\"\"Count liberties of group using flood fill\"\"\"\n        color = self.grid[row, col]\n        if color == 0:\n            return 0\n        \n        visited = np.zeros_like(self.grid, dtype=bool)\n        liberty_count = 0\n        stack = [(row, col)]\n        \n        while stack:\n            r, c = stack.pop()\n            if visited[r, c]:\n                continue\n            visited[r, c] = True\n            \n            for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n                nr, nc = r + dr, c + dc\n                if not self._is_on_grid(nr, nc):\n                    continue\n                    \n                if self.grid[nr, nc] == 0:\n                    liberty_count += 1\n                elif self.grid[nr, nc] == color and not visited[nr, nc]:\n                    stack.append((nr, nc))\n        \n        return liberty_count\n    \n    def _remove_group(self, row, col):\n        \"\"\"Remove a captured group\"\"\"\n        color = self.grid[row, col]\n        stack = [(row, col)]\n        visited = set()\n        \n        while stack:\n            r, c = stack.pop()\n            if (r, c) in visited:\n                continue\n            visited.add((r, c))\n            \n            if not self._is_on_grid(r, c) or self.grid[r, c] != color:\n                continue\n            \n            self.grid[r, c] = 0\n            point = Point(row=r+1, col=c+1)\n            self._hash ^= HASH_CODE[point, color]\n            \n            for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n                stack.append((r + dr, c + dc))\n    \n    def zobrist_hash(self):\n        return self._hash\n    \n    def get(self, row, col):\n        return self.grid[row, col]\n\n\nclass FastGameState:\n    \"\"\"Optimized game state\"\"\"\n    def __init__(self, board, next_player, previous_hash=None, last_move = None, second_last_move = None):\n        self.board = board\n        self.next_player = next_player  # 1 for black, 2 for white\n        self.previous_hashes = set()\n        if previous_hash is not None:\n            self.previous_hashes.add(previous_hash)\n        self.last_move = last_move\n        self.second_last_move = second_last_move\n    \n    def apply_move(self, move):\n        \"\"\"Apply move and return new state (only copies when needed)\"\"\"\n        if move.is_play:\n            point = move.point\n            row = point.row - 1\n            col = point.col - 1\n            new_board = self.board.copy()  # Fast numpy copy\n            new_board.place_stone(self.next_player, row, col)\n        else:\n            new_board = self.board.copy()\n        \n        new_state = FastGameState(new_board, 3 - self.next_player, \n                                  self.board.zobrist_hash(), move, self.last_move)\n        new_state.previous_hashes = self.previous_hashes.copy()\n        new_state.previous_hashes.add(self.board.zobrist_hash())\n        \n        return new_state\n\n    def is_over(self):\n        if self.last_move is None:\n            return False\n        if self.last_move.is_resign:\n            return True\n        if self.second_last_move is None:\n            return False\n        return self.last_move.is_pass and self.second_last_move.is_pass\n    \n    def is_valid_move(self, move):\n        \"\"\"Check if move is valid\"\"\"\n        if self.is_over():\n            return False\n        if move.is_pass or move.is_resign:\n            return True\n\n        point = move.point\n        row = point.row - 1\n        col = point.col - 1\n\n        if self.board.get(row, col) != 0:\n            return False\n        \n        # Quick check: simulate move\n        test_board = self.board.copy()\n        test_board.place_stone(self.next_player, row, col)\n        \n        # Check for self-capture\n        if test_board._count_liberties(row, col) == 0:\n            return False\n        \n        # Check for ko violation\n        if test_board.zobrist_hash() in self.previous_hashes:\n            return False\n        \n        return True\n    \n    def legal_moves(self):\n        \"\"\"Return all legal moves as (row, col) tuples\"\"\"\n        legal = []\n        for row in range(self.board.num_rows):\n            for col in range(self.board.num_cols):\n                move = Move.play(Point(row+1, col+1))\n                if self.is_valid_move(move):\n                    legal.append(move)\n        legal.append(Move.pass_turn())\n        legal.append(Move.resign())\n        return legal\n\n    def winner(self):\n        if not self.is_over():\n            return None\n        if self.last_move.is_resign:\n            return self.next_player\n        game_result = compute_game_result(self)\n        return game_result.winner\n    \n    @classmethod\n    def new_game(cls, board_size=19):\n        board = FastBoard(board_size, board_size)\n        return FastGameState(board, 1)  # Black plays first","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FastTerritory:\n    \"\"\"Fast territory evaluation results\"\"\"\n    __slots__ = ['num_black_territory', 'num_white_territory', \n                 'num_black_stones', 'num_white_stones', \n                 'num_dame', 'dame_points']\n    \n    def __init__(self):\n        self.num_black_territory = 0\n        self.num_white_territory = 0\n        self.num_black_stones = 0\n        self.num_white_stones = 0\n        self.num_dame = 0\n        self.dame_points = []\n\nclass FastGameResult:\n    \"\"\"Game result with score calculation\"\"\"\n    __slots__ = ['b', 'w', 'komi']\n    \n    def __init__(self, b, w, komi=7.5):\n        self.b = b\n        self.w = w\n        self.komi = komi\n    \n    @property\n    def winner(self):\n        \"\"\"Return 1 for black, 2 for white, 0 for tie\"\"\"\n        if self.b > self.w + self.komi:\n            return 1  # Black\n        if self.b < self.w + self.komi:\n            return 2  # White\n        return 0  # Tie\n    \n    @property\n    def winning_margin(self):\n        w = self.w + self.komi\n        return abs(self.b - w)\n    \n    def __str__(self):\n        w = self.w + self.komi\n        if self.b > w:\n            return f'B+{self.b - w:.1f}'\n        return f'W+{w - self.b:.1f}'\n\ndef evaluate_territory_fast(board):\n    \"\"\"Optimized territory evaluation using numpy and flood fill\n    \n    Returns Territory object with stone and territory counts\n    \"\"\"\n    grid = board.grid\n    rows, cols = grid.shape\n    \n    # Status: 0=unvisited, 1=black stone, 2=white stone, \n    # 3=black territory, 4=white territory, 5=dame\n    status = np.zeros((rows, cols), dtype=np.int8)\n    status[grid > 0] = grid[grid > 0]  # Copy stones\n    \n    territory = Territory()\n    \n    # Count stones directly from grid\n    territory.num_black_stones = np.sum(grid == 1)\n    territory.num_white_stones = np.sum(grid == 2)\n    \n    # Flood fill empty regions\n    for r in range(rows):\n        for c in range(cols):\n            if status[r, c] == 0:  # Empty and unvisited\n                points, borders = _collect_region_fast(r, c, grid, status)\n                \n                # Determine territory ownership\n                if len(borders) == 1:\n                    # Single color border = territory\n                    owner = borders.pop()\n                    if owner == 1:\n                        territory.num_black_territory += len(points)\n                        fill_value = 3\n                    else:\n                        territory.num_white_territory += len(points)\n                        fill_value = 4\n                else:\n                    # Multiple colors or no border = dame\n                    territory.num_dame += len(points)\n                    territory.dame_points.extend(points)\n                    fill_value = 5\n                \n                # Mark all points in region\n                for pr, pc in points:\n                    status[pr, pc] = fill_value\n    \n    return territory\n\n\ndef _collect_region_fast(start_r, start_c, grid, status):\n    \"\"\"Fast flood fill using stack instead of recursion\n    \n    Args:\n        start_r, start_c: Starting position\n        grid: Board grid (numpy array)\n        status: Status tracking array\n        \n    Returns:\n        (points, borders): List of (row, col) tuples and set of border colors\n    \"\"\"\n    rows, cols = grid.shape\n    points = []\n    borders = set()\n    \n    # Stack-based flood fill (much faster than recursion)\n    stack = [(start_r, start_c)]\n    visited = np.zeros((rows, cols), dtype=bool)\n    \n    while stack:\n        r, c = stack.pop()\n        \n        if visited[r, c]:\n            continue\n        visited[r, c] = True\n        \n        points.append((r, c))\n        \n        # Check all 4 neighbors\n        for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n            nr, nc = r + dr, c + dc\n            \n            if not (0 <= nr < rows and 0 <= nc < cols):\n                continue\n            \n            neighbor_value = grid[nr, nc]\n            \n            if neighbor_value == 0 and not visited[nr, nc]:\n                # Empty space, continue flood fill\n                stack.append((nr, nc))\n            elif neighbor_value > 0:\n                # Stone found, add to borders\n                borders.add(neighbor_value)\n    \n    return points, borders\n\n\ndef compute_game_result_fast(game_state, komi=7.5):\n    \"\"\"Compute final game result\n    \n    Args:\n        game_state: FastGameState object\n        komi: Komi value (default 7.5 for standard rules)\n        \n    Returns:\n        GameResult object\n    \"\"\"\n    territory = evaluate_territory_fast(game_state.board)\n    \n    black_score = territory.num_black_territory + territory.num_black_stones\n    white_score = territory.num_white_territory + territory.num_white_stones\n    \n    return GameResult(black_score, white_score, komi)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport time\nclass FastRandomBot:\n    def __init__(self, player):\n        self.player = player\n    def play_move(self, game_state):\n        legal_moves = game_state.legal_moves()\n        random_move = random.choice(legal_moves)\n        new_game_state = game_state.apply_move(random_move)\n        return new_game_state, 3 - self.player\n\ndef play_game():\n    game_state = FastGameState.new_game()\n    \n    agent = FastRandomBot(player = 1)\n    opponent = FastRandomBot(player = 2)\n    next_player = 1\n    while not game_state.is_over():\n        if next_player == 1:\n            game_state, next_player = agent.play_move(game_state)\n        else:\n            game_state, next_player = opponent.play_move(game_state)\n    return game_state.winner()\n\nstart = time.time()\nfor _ in range(10):\n    winner = play_game()\n    print(winner)\nend = time.time()\n\nprint(f\"time elapsed {end - start} seconds\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4 Plane Encoder\n\nOriginal AlphaGo uses a 48 plane encoder, we use 4 plane encoder, which is also mentioned in the paper.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n\"\"\"\nFeature name            num of planes   Description\nStone colour            3               Player stone / opponent stone / empty\nOnes                    1               A constant plane filled with 1\n\"\"\"\n\nFEATURE_OFFSETS = {\n    \"stone_color\": 0,\n    \"ones\": 3,\n    \"current_player_color\": 4,\n    \"legal_moves\": 5\n}\n\n\ndef offset(feature):\n    return FEATURE_OFFSETS[feature]\n\n# TODO: I need to change the Four plane encoder, just because I have changed the environment\nclass FourplaneEncoder:\n    def __init__(self, board_size=(19, 19), use_player_plane=True, use_legal_moves=True):\n        self.board_width, self.board_height = board_size\n        self.use_player_plane = use_player_plane\n        self.use_legal_moves = use_legal_moves\n        self.num_planes = 4 + use_player_plane + use_legal_moves\n\n    def name(self):\n        return 'fourplane'\n\n    def encode(self, game_state):\n        board_tensor = np.zeros((self.num_planes, self.board_height, self.board_width))\n        \n        # Set empty cells plane (default to empty)\n        board_tensor[offset(\"stone_color\") + 2] = 1\n        \n        # Iterate over occupied points only (much faster for sparse boards)\n        next_player = game_state.next_player\n        opponent = next_player.other\n        stone_color_offset = offset(\"stone_color\")\n        \n        for point, go_string in game_state.board._grid.items():\n            if go_string is None:\n                continue\n            r = point.row - 1\n            c = point.col - 1\n            if go_string.color == next_player:\n                board_tensor[stone_color_offset][r][c] = 1\n                board_tensor[stone_color_offset + 2][r][c] = 0  # Not empty\n            elif go_string.color == opponent:\n                board_tensor[stone_color_offset + 1][r][c] = 1\n                board_tensor[stone_color_offset + 2][r][c] = 0  # Not empty\n        \n        # Set ones plane once (moved outside loop)\n        board_tensor[offset(\"ones\")] = 1\n        \n        # Set player plane once (moved outside loop)\n        if self.use_player_plane and next_player == Player.black:\n            board_tensor[offset(\"current_player_color\")] = 1\n        \n        # Set legal moves - optimized inline computation to avoid expensive deep copies\n        if self.use_legal_moves:\n            if not game_state.is_over():\n                legal_moves_offset = offset(\"legal_moves\")\n                board = game_state.board\n                \n                # Fast inline legal move checking without deep copies\n                for row in range(1, board.num_rows + 1):\n                    for col in range(1, board.num_cols + 1):\n                        point = Point(row, col)\n                        \n                        # Fast check: point must be empty\n                        if board._grid.get(point) is not None:\n                            continue\n                        \n                        # Check self-capture without deep copy\n                        has_liberty = False\n                        would_capture = False\n                        friendly_strings = []\n                        \n                        for neighbor in point.neighbors():\n                            if not board.is_on_grid(neighbor):\n                                continue\n                            neighbor_string = board._grid.get(neighbor)\n                            if neighbor_string is None:\n                                has_liberty = True\n                                break\n                            elif neighbor_string.color == next_player:\n                                friendly_strings.append(neighbor_string)\n                            else:  # opponent string\n                                if neighbor_string.num_liberties == 1:\n                                    would_capture = True\n                        \n                        # If has liberty, not self-capture\n                        if not has_liberty:\n                            # Check if all friendly strings would have no liberties\n                            if friendly_strings and all(s.num_liberties == 1 for s in friendly_strings):\n                                if not would_capture:\n                                    continue  # Self-capture, skip\n                        \n                        # Check ko violation - simplified heuristic for performance\n                        # Full ko check requires deep copy, so we use a fast heuristic:\n                        # If last move captured exactly one stone and we're trying to recapture\n                        # at that same position, it's likely a ko violation\n                        if would_capture and game_state.last_move and game_state.last_move.is_play:\n                            # Check if we're trying to play at the last move position\n                            # (which would be recapturing after a single-stone capture)\n                            if point == game_state.last_move.point:\n                                # Count how many stones we'd capture\n                                captured_stones = 0\n                                for neighbor in point.neighbors():\n                                    if not board.is_on_grid(neighbor):\n                                        continue\n                                    neighbor_string = board._grid.get(neighbor)\n                                    if neighbor_string and neighbor_string.color == opponent:\n                                        if neighbor_string.num_liberties == 1:\n                                            captured_stones += len(neighbor_string.stones)\n                                # If capturing exactly one stone at last move position, likely ko\n                                if captured_stones == 1:\n                                    continue  # Likely ko violation, skip\n                        \n                        # Valid move - set it\n                        r = row - 1\n                        c = col - 1\n                        board_tensor[legal_moves_offset][r][c] = 1\n\n        return board_tensor\n\n    def ones(self):\n        return np.ones((1, self.board_height, self.board_width))\n\n    def zeros(self):\n        return np.zeros((1, self.board_height, self.board_width))\n\n    def encode_point(self, point):\n        return self.board_width * (point.row - 1) + (point.col - 1)\n\n    def decode_point_index(self, index):\n        row = index // self.board_width\n        col = index % self.board_width\n        return Point(row=row + 1, col=col + 1)\n\n    def num_points(self):\n        return self.board_width * self.board_height\n\n    def shape(self):\n        return self.num_planes, self.board_height, self.board_width\n\n\ndef create(board_size):\n    return FourplaneEncoder(board_size)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Supervised Learning Policy Network\n\n - Convolution layer with rectifier non-linearities.\n - CNN with 13 layers\n - Final softmax applied to legal moves\n - Data has been split into 4:1 train to test ratio. (In AlphaGo it is 28:1)\n - Pass moves has been excluded from the dataset\n - All 8 reflections and rotations has been applied and precomputed. We randomly sample mini batch from the augmented sample\n - Asynchronous stochastic gradient descent to minimize the log likelihood\n - Learning parameter is initialized to 0.03 and halved every 30k steps. (In AlphaGo, learning rate is 0.003 and halved every 80 mil training steps)\n - mini batch size of 16 as mentioned in the paper\n - zero momentum","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\nclass SLPolicyNetwork(nn.Module):\n    def __init__(self, features=5, filters=192):\n        super(SLPolicyNetwork, self).__init__()\n        self.features = features\n        self.filters = filters\n        self.first_layer = nn.Conv2d(self.features, self.filters, kernel_size=5, stride=1, padding=2)\n\n        self.hidden_layers = nn.ModuleList([\n            nn.Conv2d(self.filters, self.filters, kernel_size=3, stride=1, padding=1)\n            for _ in range(11)\n        ])\n\n        self.final_layer = nn.Conv2d(self.filters, 1, kernel_size=1, stride=1)\n\n    def forward(self, x):\n        # extract legal moves from the data tensor\n        legal_moves = x[:, 5, :, :]\n        x = F.relu(self.first_layer(x))\n        for layer in self.hidden_layers:\n            x = F.relu(layer(x))\n        # Logits are the unnormalized scores output by the network for each possible move before softmax\n        policy_logits = self.final_layer(x)  # (batch, 1, board_size, board_size)\n\n        # apply legal move mask\n        # Reshape and apply log_softmax for NLLLoss compatibility as per Alphago paper\n        batch_size = policy_logits.size(0)\n        policy_logits = policy_logits.view(batch_size, -1)\n        legal_move_mask = legal_moves.view(batch_size, -1)\n        # apply log softmax to only legal moves\n        masked_policy_logits = torch.where(\n            legal_move_mask.bool(),\n            policy_logits,\n            torch.full_like(policy_logits, float('-inf'))\n        )\n        log_probs = F.log_softmax(masked_policy_logits, dim=1)\n        return log_probs","metadata":{"id":"ZdWFRyo85o6w","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Supervised Learning Policy Trainer","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\n\nclass SLPolicyTrainer:\n    def __init__(self, model):\n        # initialize hyperparams\n        self.model = model\n        self.optimizer = None\n        self.criterion = None\n        self.scheduler = None\n\n    def initialize(self):\n        self.optimizer = optim.SGD(\n            params = self.model.parameters(),\n            lr = 0.03\n        )\n        self.scheduler = StepLR(self.optimizer, step_size=30000, gamma=0.5)\n        self.criterion = nn.NLLLoss()\n        return\n    def train(self, loader):\n        self.model.train()\n        total_loss = 0\n        correct_predictions = 0\n        total_sample = 0\n        for (x, y) in loader:\n            # Cast input to float32\n            x = x.to(torch.float32)\n            self.optimizer.zero_grad()\n            log_probs = self.model(x)\n\n            loss = self.criterion(log_probs, y)\n\n            loss.backward()\n            self.optimizer.step()\n            self.scheduler.step()\n\n            total_loss += loss.item()\n            total_sample += y.size(0)\n            prediction = log_probs.argmax(dim=1)\n            correct_predictions += (prediction == y).sum().item()\n\n        avg_loss = total_loss / len(loader)\n        acc = correct_predictions / total_sample\n\n        return avg_loss, acc\n    def evaluate(self, loader):\n        self.model.eval()\n        total_loss = 0\n        total_sample = 0\n        correct_predictions = 0\n        with torch.no_grad():\n            for (x, y) in loader:\n                # Cast input to float32\n                x = x.to(torch.float32)\n                log_probs = self.model(x)\n                loss = self.criterion(log_probs, y)\n                prediction = log_probs.argmax(dim=1)\n                total_loss += loss.item()\n                total_sample += y.size(0)\n                correct_predictions += (prediction == y).sum().item()\n\n\n        avg_loss = total_loss / len(loader)\n        acc = correct_predictions / total_sample\n        return avg_loss, acc","metadata":{"id":"rc-b32Xl7sHa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DataLoader for Go","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.utils.data as td\n\n__all__ = [\n    'GoDataLoader'\n]\n\nclass GoDataLoader:\n    def __init__(self, feature_path, label_path = None):\n        self.feature_path = feature_path\n        self.label_path = label_path\n\n    def load_data(self):\n        features = np.load(self.feature_path)\n        features_tensor = torch.from_numpy(features)\n\n        if self.label_path:\n            labels = np.load(self.label_path)\n            labels_tensor = torch.from_numpy(labels).to(torch.int64) # Cast labels to torch.int64\n            # this dataset can be directly used with torch's DataLoader\n            dataset = td.TensorDataset(features_tensor, labels_tensor)\n        else:\n            dataset = td.TensorDataset(features_tensor)\n        return dataset","metadata":{"id":"fPmtmIWR7zXz","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Loop of Supervised Learning Policy","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\n\nprint(\"Load model\")\nmodel = SLPolicyNetwork(features=6)\nnum_of_epochs = 10\n\n# prepare the dataset\nprint(\"fetch and load data\")\ntraining_feature_path = '/kaggle/input/alphago-kgs-200k/KGS-2019_04-19-1255-_train_features.npy'\ntraining_label_path = '/kaggle/input/alphago-kgs-200k/KGS-2019_04-19-1255-_train_labels.npy'\ntest_feature_path = '/kaggle/input/alphago-kgs-200k/KGS-2019_04-19-1255-_test_features.npy'\ntest_label_path = '/kaggle/input/alphago-kgs-200k/KGS-2019_04-19-1255-_test_labels.npy'\ntraining_dataset = GoDataLoader(training_feature_path, training_label_path).load_data()\ntest_dataset = GoDataLoader(test_feature_path, test_label_path).load_data()\n\n# load the dataset to a loader\n# this is what we will pass to our Trainer\ntraining_loader = DataLoader(training_dataset, batch_size=16, shuffle=True, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n\n# load the trainer\ntrainer = SLPolicyTrainer(model)\ntrainer.initialize()\n\nfor epoch in range(num_of_epochs):\n    training_loss, training_acc = trainer.train(training_loader)\n    test_loss, test_acc = trainer.evaluate(test_loader)\n    print(f\"epoch: {epoch}, training loss: {training_loss}, training acc: {training_acc}, test loss: {test_loss}, test_acc: {test_acc}\")\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': trainer.optimizer.state_dict(),\n        'train_acc': training_acc,\n        'test_acc': test_acc,\n    }, f'sl_policy_epoch_{epoch}.pth')","metadata":{"id":"EvlY8kbo78Ui","outputId":"5f299c25-e68a-4dd0-917e-9a7f1d1f526f","trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check if gpu is available\n\nimport torch\n\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA device count: {torch.cuda.device_count()}\")\nif torch.cuda.is_available():\n    print(f\"Current CUDA device: {torch.cuda.current_device()}\")\n    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Supervised Learning Policy Agent","metadata":{}},{"cell_type":"code","source":"import torch\n\nclass SLPolicyAgent:\n    def __init__(self, model):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model = model.to(self.device)\n        self.encoder = FourplaneEncoder()\n\n    def select_move(self, game_state):\n        assert isinstance(game_state, GameState)\n        encoded_game_state = self.encoder.encode(game_state)\n        x = torch.from_numpy(encoded_game_state)\n        x = x.unsqueeze(0)\n        x = x.to(torch.float32)\n        x = x.to(self.device)\n        log_probs = self.model(x)\n        log_probs = log_probs.squeeze(0)\n        legal_moves = game_state.legal_moves()\n        legal_moves = legal_moves[:-2]\n        if len(legal_moves) == 0:\n            return None, float('-inf')\n\n        legal_moves_mask = torch.zeros(log_probs.size(0), dtype=torch.bool, device=self.device)\n        \n        for move in legal_moves:\n            if move is not None and move.is_play:\n                encoded_point = self.encoder.encode_point(move.point)\n                legal_moves_mask[encoded_point] = True\n\n        log_probs = log_probs.masked_fill(~legal_moves_mask, float('-inf'))\n        action = torch.argmax(log_probs)\n        log_prob = log_probs[action]\n        return action, log_prob\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Reinforcement learning of Policy Network\n\n - We use the same network as for Supervised Learning\n - We initialize the policy to be theta ( which is the supervised learning model)\n - We also use an opponent pool to randomly select opponent for mini batch training as mentioned in the paper\n - Also, using baseline default to 0 for the first pass. AlphaGo uses value network as baseline in the second pass\n - 10k episodes of 128 mini batch size\n - We add a model to the opponent pool after every 500 episodes\n - Used REINFORCE algorithm with stochastic gradient ascent for policy updates.","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nimport torch\nimport random\n\nclass RLPolicyTrainer:\n    def __init__(self, baseline = 0.0):\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model = SLPolicyNetwork(features=6).to(self.device)\n        self.opponent_pool = []\n        self.optimizer = None\n        self.baseline = baseline\n        self.episodes = 1\n        self.mini_batch = 1\n        self.board_size = 19\n        self.encoder = FourplaneEncoder()\n\n    def initialize(self):\n        model_file = torch.load('/kaggle/input/sl-policy-epoch-9/pytorch/default/1/sl_policy_epoch_9.pth', map_location=self.device)\n        self.model.load_state_dict(model_file['model_state_dict'])\n        self.optimizer = optim.SGD(params=self.model.parameters(), lr=0.03)\n        opponent_model = SLPolicyNetwork(features=6).to(self.device)\n        opponent_model.load_state_dict(model_file['model_state_dict'])\n        self.opponent_pool.append(opponent_model)\n\n    def train_batch(self):\n        opponent_model = random.choice(self.opponent_pool)\n        opponent_model.to(self.device)\n        opponent_model.eval()\n        self.model.eval()\n\n        streams = [torch.cuda.Stream(device=self.device) for _ in range(2)]\n        \n        agent = SLPolicyAgent(self.model)\n        opponent = SLPolicyAgent(opponent_model)\n        \n        expected_reward = 0\n        total_wins = 0\n        trajectories_list = []\n        rewards_list = []\n\n        for i in range(self.mini_batch):\n            print(f\"starting game: {i+1}\")\n\n            stream = streams[i % 2]\n            with torch.cuda.stream(stream):\n                game_state = GameState.new_game(self.board_size)\n                trajectory = []\n                while not game_state.is_over():\n                    if game_state.next_player == Player.black:\n                        action, _ = agent.select_move(game_state)\n                        if action is None:\n                            # player does not have any legal moves left apart from resigning or pass\n                            move = Move.resign()\n                            point = None\n                            break\n                        point = self.encoder.decode_point_index(action.item())\n                        trajectory.append((action, game_state))\n                    else:\n                        action, log_prob = opponent.select_move(game_state)\n                        if action is None:\n                            move = Move.resign()\n                            point = None\n                            break\n                        point = self.encoder.decode_point_index(action.item())\n                        \n                        if point is not None:\n                            move = Move(point)\n                        game_state = game_state.apply_move(move)\n                winner = game_state.winner()\n                print(f\"Winner: {winner}\")\n                reward = 1 if winner == Player.black else -1\n                trajectories_list.append(trajectory)\n                rewards_list.append(reward)\n\n        torch.cuda.synchronize(device=self.device)\n\n        for trajectory, reward in zip(trajectories_list, rewards_list):\n            if reward == 1:\n                total_wins += 1\n            for (action, ) in trajectory:\n                expected_reward += (reward - self.baseline) * log_prob\n\n        expected_reward /= self.mini_batch\n        win_rate = total_wins / self.mini_batch\n        \n        return expected_reward, win_rate\n\n    def train(self):\n        for i in range(self.episodes):\n            print(f\"starting episode: {i+1}\")\n            expected_reward, win_rate = self.train_batch()\n            self.model.train()\n            self.optimizer.zero_grad()\n            expected_loss = -expected_reward\n            expected_loss.backward()\n            self.optimizer.step()\n\n            torch.cuda.empty_cache()\n            \n            print(f\"expected loss: {expected_loss}, accuracy: {win_rate}\")\n            \n            if i % 500 == 0:\n                opponent_model = SLPolicyNetwork(features=6)\n                opponent_model.load_state_dict(self.model.state_dict())\n                self.opponent_pool.append(opponent_model)\n                \n        \n                    \n\nrl_trainer = RLPolicyTrainer()\nrl_trainer.initialize()\nrl_trainer.train()\n        \n        \n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}